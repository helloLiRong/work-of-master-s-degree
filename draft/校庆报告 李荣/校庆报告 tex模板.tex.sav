 \documentclass[10pt]{cctart}
 \usepackage{amsfonts}
 \setcounter{page}{1} % The number of 1st page is 1
 \usepackage{flafter}
 \usepackage{mathbbold}
 \usepackage{mathrsfs}
 \usepackage{amsmath}
 \usepackage{amssymb}
 \usepackage{latexsym}
 \usepackage{psfrag}
 \usepackage{graphicx,subfigure}
 \usepackage{fancyhdr,graphicx}
 \usepackage{multicol}
 \usepackage{enumerate}
 \usepackage{float}

 \textheight 8.4in \textwidth 7.0in \tolerance=10000
 \parskip=3pt
 \oddsidemargin -1.2truecm \baselineskip 1true mm \overfullrule=0pt
 \topmargin -0.6in
 \renewcommand{\sectionformat}{}
 \renewcommand\refname{参考文献:}
 \oddsidemargin -1.2truecm \evensidemargin 0in \makeatletter
 \setcounter{tocdepth}{3}
 \pagestyle{plain}
 \newsavebox{\mygraphic}
 \sbox{\mygraphic}{\includegraphics[totalheight=0.65in]{head.jpg}}
 \pagestyle{fancy} \topmargin=0pt \headheight=18mm \headsep=4mm
 \fancyhead{} \fancyhead[L]{\usebox{\mygraphic}}
 \fancyhead[R]{2011东南大学校庆研究生学术报告会\\}
 \fancyfoot[R]{\footnotesize{2011东南大学校庆研究生学术报告会}}
 \makeatletter
 \let\ps@tmp=\ps@fancy
 \let\ps@plain=\ps@fancy
 \makeatother \topskip=12.0pt
 \def\vp{\varphi}
 \newcounter{local}
 \newcommand{\scl}{\stepcounter{local}}
 \newtheorem{defi}{\heiti\textbf{\ \ \quad 定义}}[section]
 \newtheorem{theo}{\heiti\textbf{\ \ \quad 定理}}[section]
 \newtheorem{lem}[theo]{\heiti\textbf{\ \ \quad 引理}}
 \newtheorem{col}[theo]{\heiti\textbf{\ \ \quad 推论}}
 \newtheorem{prop}[theo]{\heiti\textbf{\ \ \quad 命题}}
 \newtheorem{remark}[theo]{\heiti\textbf{\ \ \quad 注}}
 \newcommand{\dist}{\mathrm{dist}\mbox{}}
 \newcommand\be{\begin{equation}}
 \newcommand\ee{\end{equation}}
 \newcommand\bes{\begin{eqnarray}}
 \newcommand\ees{\end{eqnarray}}
 \newcommand\bess{\begin{eqnarray*}}
 \newcommand\eess{\end{eqnarray*}}
 \newcommand\bR{{\mathbb R}}
 \newcommand{\lbl}[1]{\label{#1}}
%\mathindent 2cm
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\baselineskip 6 true mm
\date{}
\title{\heiti 基于神经网络图灵机的算法学习的研究
\author{{\small 王李荣~~~~ 高志强}
\\ {(\footnotesize  东南大学计算机科学与工程学院, 南京, 210096)}}}

\maketitle \hskip 1cm

\ {\heiti 摘\  \ 要} \ \
如何通过输入输出样例自动学习算法一直是人工智能的一个核心问题。近年来，深度学习模型被许多研究者应用于该领域，神经网络图灵机是其中之一，神经网络图灵机是一个外部记忆增强的循环神经网络，包含控制器和外部记忆模块两个基本组件。本文使用长短时记忆网络作为模型的控制器，使用序列到序列的框架训练模型，完美的学习到加法、乘法等算法。本文展示了两项策略能改善模型的性能：设计合适课程学习策略和使用适合的外部记忆大小。

\vskip 0.2cm ~{\heiti 关键词}~ ~
LSTM; NTM; 记忆网络; 算法学习
\vskip 1.2cm
\begin{center}
{\Large\bf   The indeterminates number of a maximal-nonsingular partial matrix and some counter examples of partial $N_{0}$ matrix}
\end{center}
\vskip 0.3cm \centerline{Jie Ren ~~~~Jianlong Chen} \vskip 0.2cm
\centerline{(Department of  Mathematics,  Southeast University,
Nanjing 210096)} \vskip 0.2cm
\noindent$\mathbf{Abstract:}$\quad
In 2010,  Richard A. Brualdi ，Zejun Huang and Xingzhi Zhan released a question about how to characterize
a maximal-nonsingular partial matrix. It has been proved, useing idea of  borderd matrix, there exists
maiximal-nonsingular partial matrix of any order, with the number of the indeterminates being zero. When $n\geqslant1$,
there also exists maximal-nonsingular partialmatrix that has only one indeterminate. Then we guess that:
for a $n$ order maximal-nonsingular partial matrix, the probable numbers of its indeterminates are $0, 1, 2, \cdots, \frac{n(n-1)}{2}$.Morever, some counter examples about the completion of partial $N_{0}-$matrix are given.\\
\vskip 0.1cm \noindent$\mathbf{Key\ words:}$\quad Partial matrix；Completion；Bordered matrix；Nonsingular; $N_{0}-$partial matrix
 \vskip 0.2cm
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{multicols}{2}

\section{引\quad 言}
计算机执行一个程序，需要处理数值计算，$if$表达式，赋值等基本操作以及这一系列基本操作的组合。本文和\cite{zaremba2014learning}一样将这些基本操作称为算法，例如加法，乘法，复制都被称为算法。对于冯诺依曼体系结构的计算机来说，程序员通过编码操作CPU和寄存器可以很轻松的完成算法任务。但是如何让计算机通过输出输出样例自动学会这些算法，一直是人工智能的一个核心问题。本文中将通过输出输出样例发现算法的任务称为算法学习\cite{zaremba2014learning}。

实际上，算法学习不是一个新任务，它也被称为程序推理（Program induction），最早可以追溯到1967年\cite{gold1967language, angluin1983inductive}，是一个非常古老但却十分有意义的研究领域。这项任务有着丰富的应用场景，例如：帮助没有计算机背景的人编写程序，帮助程序员发现问题，发现新算法，帮助教学。因而在过去几十年，有许多研究者投入到该任务的研究中\cite{nordin1997evolutionary,liang2013learning,wineberg1994representation,solomonoff1964formal}，并做出了卓越的贡献。

在近些年，有研究者利用机器学习模型，尤其是循环神经网络（Recurrent Neural Network, RNN），研究该问题\cite{graves2014neural,zaremba2015reinforcement,kaiser2015neural,kurach2015neural,andrychowicz2016learning}。在这些工作之中，与本文最相关的模型是神经网络图灵机（Neural Turing Machine, NTM）\cite{graves2014neural}。NTM是一类被称为外部记忆增强的循环神经网络（Memory augment Recurrent Neural Network）。该网络结构与长短时记忆网路（Long Short Term Memory, LSTM）\cite{hochreiter1997long}有相似之处。这两种结构都是利用记忆模块解决任务中存在的长距离依赖问题，例如LSTM在语音识别\cite{amodei2016deep}，自然语言处理\cite{bahdanau2014neural}等任务中的应用。不同的是LSTM中，增加记忆单元的个数会导致模型参数个数的增加，从而使得模型难以训练。而NTM这类外部记忆增强的循环神经网络为了解决该问题，将记忆模块从网络中分离出来，将网络分割为可训练的控制器模块和外部记忆模块两个部分，使得外部记忆模块的大小将不再依赖于网络参数的个数。根据\cite{graves2014neural}的实验结果，NTM能学习到简单算法，例如复制，重复复制等。

本文发现训练NTM学习加法，乘法等算法时，NTM性能差，因此本文使用了课程学习\cite{bengio2009curriculum}策略。具体来说，本文设计了三种课程学习策略，针对每个任务分别使用三种课程学习策略进行训练，实验结果表明使用课程学习提升了模型的性能。对于算法任务，记忆模块是否被有效的利用是模型能否学习到算法任务的关键。例如：在复制算法（3.1节将具体介绍该任务）中，复制长度为20的字符序列，则要求模型有20个记忆单元（每个记忆单元存储一个字符）。如果记忆单元的个数如果少了，则模型的性能会下降，反之，多余的记忆单元则不会影响模型的性能。因此，为了验证NTM的记忆模块是否被有效地利用，本文研究了记忆模块的大小对NTM性能的影响。

综上，本文的主要工作是：
\begin{enumerate}
  \item 实现NTM，并将该模型应用于复杂的算法学习任务，包括加法，乘法等任务
  \item 研究课程学习对模型行性能的影响。实验结果表明，使用课程学习使NTM学到加法，乘法的算法。并且有更快的收敛速度。
  \item 研究外部记忆模块的大小对模型的影响。实验结果表明，选择合适的外部记忆大小，能提升模型的性能。
\end{enumerate}

\section{相关工作}
算法学习这个任务最早可以追溯到1967年\cite{gold1967language, angluin1983inductive}。在其漫长的研究过程中，根据模型不同或者方法不同，该任务有许多不同的名称，例如:合成程序（program synthesis）、程序归纳（program induction）、自动编程（automatic programming）等。但是其研究目标始终是让计算机有理解程序甚至自动编写程序的能力。\cite{gulwani2010dimensions} 和\cite{kitzelmann2009inductive}两篇文章从多个角度描述了算法学习的研究现状。

在该任务的诸多模型方法中，本文重点关注机器学习模型在算法学习任务中的应用，特别是\cite{graves2014neural}提出的NTM。这是第一次明确提出外部记忆增强的神经网络模型。根据这篇文章的实验结果，NTM能学习到复制，重复复制等简单算法。

之后陆续提出的模型，都是基于NTM进行改进或者参考了NTM的结构。例如：为了解决NTM不能并行和训练难度大等问题，\cite{kaiser2015neural}借鉴了Grid LSTM\cite{kalchbrenner2015grid}的思想，提出了一种高度并行的神经网络结构-神经网络GPU（Neural GPUs，N-GPUs）；\cite{zaremba2015reinforcement}结合增强学习算法和NTM，得到了增强学习神经网络图灵机（Reinforcement Learning neural turing machine, RL-NTM）；\cite{graves2016hybrid}提出的可微神经计算机（Differentiable Neural Computer, DNC），该模型对NTM做了三点改进。其余类似的结构还有记忆神经网络（Memory neural network, MNN）\cite{weston2014memory}、指针网络（Pointer Network, Ptr-Nets）\cite{vinyals2015pointer}、栈增强的循环神经网络（Stack-Argmented Recurrent Neural Network）\cite{joulin2015inferring}、层次记忆网络（Hierarchial Attentive Memory, HAM）\cite{andrychowicz2016learning}。

这些模型从并行性、外部记忆结构、寻址模式、寻址效率等方面对NTM做出改进或者提出了类似于NTM的外部记忆增强的循环神经网络结构。虽然这些模型在某些特定方向的性能得到了提升，但是模型的复杂度也随之提升，使得模型的训练难度加大。因此，这些改进是否有意义仍然有待商榷。对于算法学习这个任务来说，需要模型具备解决变长问题和解决长距离依赖的能力。因此，本文使用NTM作为模型。

\section{模型}

这部分将简要的介绍NTM。该模型的结构包括两个最基本的部分：控制器和外部记忆模块。图\ref{Fig:ntm_architecture}展示了一个高度抽象的NTM结构。与循环神经网络结构类似，控制器在每个时间步（time step）获得外部的一个输入，并计算得到一个输出。不同的地方在于，控制器在每个时间步都会通过读头从记忆模块读取和写头于内部的模型内部的记忆模块交互。最关键的部分在于NTM的每个部分都是可微的，所以NTM可以使用梯度下降等优化算法进行训练。

\begin{figure*}[htbp]
\centering
\includegraphics[width=4.0in]{ntm_architecture.png}
\caption{NTM模型，在每一个时间步$t$，控制器从外部获得输入$input[t]$，并计算得到一个输出$output[t]$。与此同时，控制器通过若干个的读头和写头并行的从记忆模块读取和写入数据。}
\label{Fig:ntm_architecture}
\end{figure*}

在NTM中，神经网络充当控制器的角色，可以选择前向神经网络或者循环神经网络，本文的实验使用的是LSTM\cite{hochreiter1997long}。在本节剩余部分将从读机制，写机制和寻址机制三个部分介绍NTM。

\subsection{读机制}
$M_{t}$表示在时间$t$，大小为$N\times M$的记忆矩阵，其中$N$表示记忆单元的个数，$M$表示每个记忆单元的大小。$w_{t}$表示在时间$t$通过读头输出的权重向量，其中$w_{t}$中第$i$维的元素$\omega_{t}(i)$表示第$i$个记忆单元所占的权重，并且$\omega_{t}(i)$ 满足如下约束：
\begin{equation}\label{equ:1}
  \sum_{i} \omega_{t}(i) = 1,~~~0 \leq \omega_{t}(i) \leq 1,\forall i
\end{equation}
则按照如下公式计算得到读向量
\begin{equation}\label{eqe:2}
  r_{t} \leftarrow w_{t} M_{t}
\end{equation}

\subsection{写机制}
在时间$t$，读头输出权重向量$w_{t}$，$M$维的消除向量（erase vector）$e_{t}$，$M$维的加向量（add vector）$a_{t}$，其中$e_{t}$ 的每个元素都属于区间$(0,1)$。则第$i$个记忆单元的的大小根据如下公式计算得到：
\begin{equation}\label{equ:3}
  \tilde{M}_{t}(i) \leftarrow M_{t-1}(i)[1-\omega_{t}(i)e_{t}]
\end{equation}

\begin{equation}\label{equ:4}
  M_{t}(i) \leftarrow \tilde{M}_{t}(i) + \omega_{t}(i)a_{t}
\end{equation}
\\如果存在多个读头，则将每个读头的消除向量和加向量求和得到最终消除向量和加向量，在按照公式\ref{eqe:3}和公式\ref{equ:4}进行计算。

\subsection{寻址机制}

之前研究展示了NTM的读机制和写机制，但是还没展示读机制和写机制中的权重向量$w_{t}$是如何得到的。权重向量的值结合了两种寻址机制，本节简要的将其总结为以下四个公式：
\begin{equation}\label{equ:content}
    \omega_{t}^{c} \leftarrow \frac{exp(\beta_{t}K[k_{t},M_{t}(i)])}{\sum_{j}exp(\beta_{t}K[k_{t},M_{t}(j)])}
\end{equation}

\begin{equation}\label{equ:interp}
  w_{t}^{g} \leftarrow g_{t}w_{t}^c + (1 - g_{t})w_{t-1}
\end{equation}

\begin{equation}\label{equ:shift}
  \tilde{\omega}_{t}(i) \leftarrow \sum_{j}^{N-1}\omega_{t}^{g}(j)s_{t}(i-j)
\end{equation}

\begin{equation}\label{equ:sharp}
  \omega_{t}(i) \leftarrow \frac{\tilde{\omega}_{t}(i)^{\gamma_{t}}}{\sum_{j}\tilde{\omega}_{t}(j)^{\gamma_{t}}}
\end{equation}
\\其中，公式\ref{equ:content}的$K[\cdot,\cdot]$表示余弦相似度函数：
\begin{equation}\label{equ:cosine}
  K[u,v] = \frac{u \cdot v}{\|u\|\|v\|}
\end{equation}
\\另外，公式\ref{equ:content}到公式\ref{equ:sharp}中还涉及了$k_{t}$，$\beta_{t}$，$g_{t}$，$s_{t}$，$\gamma_{t}$这五个参数。这五个参数都依赖于控制器的输出，而在\cite{graves2014neural}中只是给了部分约束条件，并没有给出明确的定义。因此本文的模型实现中，使用符合约束的如下公式给出这五个参数的定义。
\\给定模型的输出$h_{t}$，五个参数分别满足如下公式：
\begin{equation}\label{equ:k}
  k_{t} \leftarrow relu(h_{t})
\end{equation}
\begin{equation}\label{equ:b}
  \beta_{t} \leftarrow relu(h_{t})
\end{equation}
\begin{equation}\label{equ:g}
  g_{t} \leftarrow sigmoid(h_{t})
\end{equation}
\begin{equation}\label{equ:s}
  s_{t} \leftarrow softmax(h_{t})
\end{equation}
\begin{equation}\label{equ:b}
  \gamma_{t} \leftarrow 1 + relu(h_{t})
\end{equation}

\section{任务}
本文中考虑的四种算法学习任务类似于\cite{zaremba2016learning}中的任务，分别是：复制，加法，三个数的加法和一位数的乘法。具体的例子可以见图\ref{Fig:task}。需要强调的是，1）对于冯诺依曼体系结构下的计算机，程序员通过编码操作CPU和寄存器可以很轻松的完成这些任务。但是对于算法学习来说，不是通过编码解决这类问题，而是使用大量数据训练NTM，使NTM学会这些算法。2）训练模型时采用的是序列到序列（sequence-to-sequence）的框架\cite{sutskever2014sequence}，即模型每个时间步读入一个字符，当读到终止符后，模型便不在接收输入，而是每个时间步输出一个字符。在全部实验中，都以“.”作为终止符。

\begin{figure*}[htbp]
\centering
\includegraphics[width=4.0in]{task.jpg}
\caption{四个任务的例子，从左到右分别是复制，加法，三个数的加法和一位数的乘法。}\label{Fig:task}
\end{figure*}

\textbf{复制} 这个任务的目的是复制输入字符。输入序列是任意长度的字符串，并以终止符结尾。这个任务可以测试模型是否有能力存取任意长度的信息。

\textbf{加法} 这个任务的目的是对输入序列求和。这个任务要求模型：1）有能力存取任意长的信息。2）发现加法中进位的概念。3）理解一位数加法的概念。本文考虑了以下两种表示方式：
\begin{enumerate}[(1)]
\item 1 2 3 4 5 + 5 4
\item 1 2 3 4 5 + 0 0 0 5 4
\end{enumerate}
在实际的实验中发现，第一种表示方式处理比较困难，尤其是增加了课程学习的设计难度，而且第二种情况是包含第一种情况的。因此本文的实验采用的是第二种表示方式，即相加的两个数都有相同的长度。如果两个数的位数不同，则位数少的补零至两个数位数相同。之后的两个任务也是采用这种方式。

\textbf{三个数加法} 与上一个任务的不同点在于有三个数参与运算。相比于两个数的加法，难度在于进位的状态有三个（0,1,2）。

\textbf{一位数乘法} 这个任务是用一个一位数乘以一个多位数。这个任务的复杂度与加法类似，不过进位的状态可以是0到8里的任意一个。

\section{实验}

\subsection{网络参数}
NTM是可微的神经网络模型，在训练模型之前，本文首先用一小部分数据集对比了随机梯度下降算法以及其他两种改进的随机梯度下降算法RMSprop和Adam，并针对学习率、Mini-batch大小、权重初始化策略等超参数进行调优。最终在训练时采用RMSP算法和表\ref{tab:superparameter}中展示的各项超参数。在各项超参数中需要说明的是NTM的控制器使用的是LSTM，隐藏层的记忆单元的个数是128。

\begin{table*}
\caption{网络超参数}\label{tab:superparameter}
\centering
\begin{tabular}{|l|l|l|l| p{5cm}|}
\hline
\textbf{超参数} & \textbf{策略}\\ \hline

学习率 & 0.0001 \\ \hline
momentum & 0.9 \\ \hline
Mini-batch大小 & 32 \\ \hline
LSTM隐藏层单元的个数 & 128 \\ \hline
权重初始化策略 & $Uniform(-r, r)$ \\ \hline
\end{tabular}
\end{table*}

\begin{table*}
\caption{任务相关参数}\label{tab:taskparameter}
\centering
\begin{tabular}{|l|l|l|l| p{5cm}|}
\hline
\textbf{。。} & \textbf{记忆大小} & \textbf{序列长度} & \textbf{训练次数}\\ \hline

复制 & 0.0001& 0.0001& 0.0001 \\ \hline
加法 & 0.9 & 0.9 & 0.9 \\ \hline
三个数加法 & 32 & 32& 32\\ \hline
一位数乘法 & 100000  & 100000  & 100000 \\ \hline
\end{tabular}
\end{table*}

\subsection{课程学习}
课程学习是在训练模型的过程中一项重要的方法，本文使用的课程学习策略类似于\cite{zaremba2014learning}中的策略。具体来说，本文实验中使用的数据集将根据输入序列的长度划分为若干组，输入序列的长度越大，则任务的难度越大。在训练时，根据数据集的难度从易到难依次用于训练。在下面的描述中，$MIN$表示训练时最小的序列长度，$MAX$表示训练时最大的序列长度。
\\\textbf{不用课程学习} 该策略作为一个比较基准，不使用课程学习策略。即实验中每一个batch的数据都从随机的从$MIN$和$MAX$生存。
\\\textbf{单纯的课程学习} 开始时，随机生成长度属于$[1, MIN]$的序列作为训练数据集，当准确率到达给定的阈值之后（本文的实验中阈值是0.9），区间的右端点值加一，即随机生成长度属于$[1, MIN + 1]$的序列作为训练数据集。重复该过程，直到右端点的值等于$MAX$。 在实验中，使用该策略的效果要好于不用课程学习，但是并不理想。因此，下一个策略是在该策略的基础上进行改进。
\\\textbf{混合的课程学习} 该策略的过程与单纯的课程学习的策略类似，唯一的不同点在于：当区间是$[1, a]$时，不再随机从整个区间生成数据，而是按照一定的比例从$[1, a-1]$和$[a, a]$生成数据，并且大部分数据来自$[a, a]$。这个策略类似于在人类学习新知识的时候，应该可以新知识为主，并需要复习旧的知识。在实验中，该策略的表现要优于其他两个策略。

\subsection{实验结果}
本文的程序基于深度学习框架tensorflow，代码托管在GitHub。实验中使用的机器是美团云服务器，配置是8核的CPU，64G内存，100G固态硬盘和1核的Tesla M60 GPU。实验中的模型是NTM，各项超参数如表\ref{tab:superparameter}所示。实验任务图\ref{Fig:task}所示。对每个任务，本文都独立训练了一个模型。复制任务训练50000个batch，加法任务训练150000个batch，三位数加法任务训练200000 个batch，乘法任务训练了300000个batch。每个任务都分别使用三种课程学习策略训练一次。针对加法任务，使用多种不同的记忆模块大小训练了多次。

\begin{figure*}[tb]
\subfigure[]{
\begin{minipage}[t]{0.23\textwidth}%并排放两张图片，每张占页面的0.5，下同。
\centering
\includegraphics[width=\textwidth]{copy.png}
\end{minipage}
}
\subfigure[]{
\begin{minipage}[t]{0.23\textwidth}
\centering
\includegraphics[width=\textwidth]{add.png}
\end{minipage}
}
\subfigure[]{
\begin{minipage}[t]{0.23\textwidth}%并排放两张图片，每张占页面的0.5，下同。
\centering
\includegraphics[width=\textwidth]{multip.png}
\end{minipage}
}
\subfigure[]{
\begin{minipage}[t]{0.23\textwidth}
\centering
\includegraphics[width=\textwidth]{copy.png}
\end{minipage}
}
\caption{三种课程学习在测试集上的准确率的对比。在各个任务中，混合的课程学习策略表现最好，没有使用课程学习表现最差。}\label{fig:result}
\end{figure*}

为了比较课程学习对模型性能的影响，本文对所有任务都绘制了准确率随训练轮数变化情况的曲线图，如图\ref{fig:result}所示。准确率的定义是模型预测准确的测试样例占全部测试样例中的比例。需要注意的是，对于一个测试样例，模型会输出一个预测序列，只有整个序列都正确，才认为这个测试样例的预测是正确的。只要一个时间步的预测是不正确的，那么这个预测就是错误的。

从图\ref{fig:result}可以看出，NTM在每个任务上都能收敛，并且达到接近100\%的准确率。其中，混合的课程学习策略使模型收敛的更快。例如，在加法任务中，使用混合课程学习策略的模型在12000个batch达到99\%的准确率；使用单纯的课程学习策略的模型在14000batch达到99\%的准确率；不使用的课程学习策略的模型在15000个batch只达到90\%的准确率。在乘法任务中，课程学习的作用表现的更加明显。。。。而在简单任务中，例如复制任务，课程学习的作用则显得比较微弱。

\section{结论}
本文研究并实现了NTM，并将NTM应用于算法学习任务。通过实验说明了NTM可以学习到加法，一位数乘法等算法。此外，实验结果表明合适的课程学习策略有助于提升模型在算法学习任务上的性能。但是仍然有许多问题值得更进一步的研究，例如：1）当测试序列的长度大于训练序列的最大长度时，NTM无法作出正确的预测。2）当学习一个新算法时，都需要重新训练一个模型，能否训练一个模型可以学习多种算法。3)NTM能否存在循环神经网络的梯度消失或者梯度爆炸等问题。
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  参考文献
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\small
\bibliographystyle{unsrt}
\bibliography{reference}

\end{multicols}
\end{document}
