 \documentclass[10pt]{cctart}
 \usepackage{amsfonts}
 \setcounter{page}{1} % The number of 1st page is 1
 \usepackage{flafter}
 \usepackage{mathbbold}
 \usepackage{mathrsfs}
 \usepackage{amsmath}
 \usepackage{amssymb}
 \usepackage{latexsym}
 \usepackage{psfrag}
 \usepackage{graphicx,subfigure}
 \usepackage{fancyhdr,graphicx}
 \usepackage{multicol}
 \usepackage{enumerate}
 \usepackage{float}
 \usepackage{enumitem}
 \usepackage{diagbox} % 加载宏包
 \setenumerate[1]{itemsep=0pt,partopsep=0pt,parsep=\parskip,topsep=5pt}
 \setitemize[1]{itemsep=0pt,partopsep=0pt,parsep=\parskip,topsep=5pt}
 \setdescription{itemsep=0pt,partopsep=0pt,parsep=\parskip,topsep=5pt}

 \textheight 8.4in \textwidth 7.0in \tolerance=10000
 \parskip=3pt
 \oddsidemargin -1.2truecm \baselineskip 1true mm \overfullrule=0pt
 \topmargin -0.6in
 \renewcommand{\sectionformat}{}
 \renewcommand\refname{参考文献:}
 \oddsidemargin -1.2truecm \evensidemargin 0in \makeatletter
 \setcounter{tocdepth}{3}
 \pagestyle{plain}
 \newsavebox{\mygraphic}
 \sbox{\mygraphic}{\includegraphics[totalheight=0.65in]{head.jpg}}
 \pagestyle{fancy} \topmargin=0pt \headheight=18mm \headsep=4mm
 \fancyhead{} \fancyhead[L]{\usebox{\mygraphic}}
 \fancyhead[R]{2011东南大学校庆研究生学术报告会\\}
 \fancyfoot[R]{\footnotesize{2011东南大学校庆研究生学术报告会}}
 \makeatletter
 \let\ps@tmp=\ps@fancy
 \let\ps@plain=\ps@fancy
 \makeatother \topskip=12.0pt
 \def\vp{\varphi}
 \newcounter{local}
 \newcommand{\scl}{\stepcounter{local}}
 \newtheorem{defi}{\heiti\textbf{\ \ \quad 定义}}[section]
 \newtheorem{theo}{\heiti\textbf{\ \ \quad 定理}}[section]
 \newtheorem{lem}[theo]{\heiti\textbf{\ \ \quad 引理}}
 \newtheorem{col}[theo]{\heiti\textbf{\ \ \quad 推论}}
 \newtheorem{prop}[theo]{\heiti\textbf{\ \ \quad 命题}}
 \newtheorem{remark}[theo]{\heiti\textbf{\ \ \quad 注}}
 \newcommand{\dist}{\mathrm{dist}\mbox{}}
 \newcommand\be{\begin{equation}}
 \newcommand\ee{\end{equation}}
 \newcommand\bes{\begin{eqnarray}}
 \newcommand\ees{\end{eqnarray}}
 \newcommand\bess{\begin{eqnarray*}}
 \newcommand\eess{\end{eqnarray*}}
 \newcommand\bR{{\mathbb R}}
 \newcommand{\lbl}[1]{\label{#1}}
 \newcommand{\upcite}[1]{\textsuperscript{\textsuperscript{\cite{#1}}}}

%\mathindent 2cm
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\makeatletter
\newenvironment{tablehere}
  {\def\@captype{table}}
  {}

\newenvironment{figurehere}
  {\def\@captype{figure}}
  {}
\makeatother

\begin{document}
\baselineskip 6 true mm
\date{}
\title{\heiti 基于神经网络图灵机的算法学习的研究
\author{{\small 王李荣~~~~ 高志强}
\\ {(\footnotesize  东南大学计算机科学与工程学院, 南京, 210096)}}}

\maketitle \hskip 1cm

\ {\heiti 摘\  \ 要} \ \
如何通过输入输出样例自动学习算法一直是人工智能的一个核心问题。近年来，深度学习模型被许多研究者应用于该领域，神经网络图灵机是其中之一，神经网络图灵机是一个外部记忆增强的循环神经网络，包含控制器和外部记忆模块两个基本组件。本文使用长短时记忆网络作为模型的控制器，使用序列到序列的框架训练模型，完美的学习到加法、乘法等算法。本文展示了两项策略能改善模型的性能：设计合适课程学习策略和使用适合的外部记忆大小。

\vskip 0.2cm ~{\heiti 关键词}~ ~
LSTM; NTM; 记忆网络; 算法学习
\vskip 1.2cm
\begin{center}
{\Large\bf   The indeterminates number of a maximal-nonsingular partial matrix and some counter examples of partial $N_{0}$ matrix}
\end{center}
\vskip 0.3cm \centerline{Jie Ren ~~~~Jianlong Chen} \vskip 0.2cm
\centerline{(Department of  Mathematics,  Southeast University,
Nanjing 210096)} \vskip 0.2cm
\noindent$\mathbf{Abstract:}$\quad
In 2010,  Richard A. Brualdi ，Zejun Huang and Xingzhi Zhan released a question about how to characterize
a maximal-nonsingular partial matrix. It has been proved, useing idea of  borderd matrix, there exists
maiximal-nonsingular partial matrix of any order, with the number of the indeterminates being zero. When $n\geqslant1$,
there also exists maximal-nonsingular partialmatrix that has only one indeterminate. Then we guess that:
for a $n$ order maximal-nonsingular partial matrix, the probable numbers of its indeterminates are $0, 1, 2, \cdots, \frac{n(n-1)}{2}$.Morever, some counter examples about the completion of partial $N_{0}-$matrix are given.\\
\vskip 0.1cm \noindent$\mathbf{Key\ words:}$\quad Partial matrix；Completion；Bordered matrix；Nonsingular; $N_{0}-$partial matrix
 \vskip 0.2cm
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{multicols}{2}

\section{引\quad 言}
计算机执行一个程序，需要处理数值计算，$if$表达式，赋值等基本操作以及这一系列基本操作的组合。Zareba等人\upcite{zaremba2014learning}将这些基本操作称为算法，例如复制，加法，乘法都被称为算法。对于冯诺依曼体系结构的计算机来说，程序员通过编码操作CPU和寄存器可以很轻松的完成算法任务。但是如何让计算机通过输出输出样例自动学会这些算法，一直是人工智能的一个核心问题，该任务被称为算法学习\upcite{zaremba2014learning}。

实际上，算法学习不是一个新任务，它也被称为程序推理（Program induction），最早可以追溯到1967年\upcite{gold1967language, angluin1983inductive}，是一个非常古老但却十分有意义的研究领域。这项任务有着丰富的应用场景，例如：帮助没有计算机背景的人编写程序，帮助程序员发现问题，发现新算法，帮助教学。因而在过去几十年，有许多研究者投入到该任务的研究中\upcite{nordin1997evolutionary,liang2013learning,wineberg1994representation,solomonoff1964formal}，并做出了卓越的贡献。

在近些年，有研究者利用机器学习模型，尤其是循环神经网络（Recurrent Neural Network, RNN），研究该问题\upcite{graves2014neural,zaremba2015reinforcement,kaiser2015neural,kurach2015neural,andrychowicz2016learning}。在这些工作之中，与本文最相关的模型是神经网络图灵机（Neural Turing Machine, NTM）\upcite{graves2014neural}。NTM是一类被称为外部记忆增强的循环神经网络（Memory augment Recurrent Neural Network）。该网络结构与长短时记忆网路（Long Short Term Memory, LSTM）\upcite{hochreiter1997long}有相似之处。这两种结构都是利用记忆模块解决任务中存在的长距离依赖问题，不同的是LSTM中，增加记忆单元的个数会导致模型参数个数的增加，从而使得模型难以训练。而NTM这类外部记忆增强的循环神经网络为了解决该问题，将记忆模块从网络中分离出来，将网络分割为可训练的控制器模块和外部记忆模块两个部分，使得外部记忆模块的大小将不再依赖于网络参数的个数。根据Graves等人\upcite{graves2014neural}的实验结果，NTM能学习到简单算法，例如复制，重复复制等。

本文发现训练NTM学习加法，乘法等算法时，NTM性能较差，因此本文使用了Bengio提出的普通的课程学习策略\upcite{bengio2009curriculum}和Zaremba\upcite{zaremba2014learning}提出的混合课程学习策略效果都不理想。因此，本文基于这两种课程学习策略提出了一种新的课程学习策略，改善了模型的性能并且加快了模型的收敛速度。

本文的研究中关注的另一个问题是外部记忆模块的大小是否被有效的利用。例如：在复制算法（3.1节将具体介绍该任务）中，复制长度为20的字符序列，理论上只有20个记忆单元（每个记忆单元存储一个字符）。如果记忆单元的个数如果少了，则模型的性能会下降，反之，多余的记忆单元则不会影响模型的性能（多余的记忆单元可以不使用）。因此，为了验证NTM的记忆模块是否被有效地利用，本文研究了记忆模块的大小对NTM 性能的影响。

综上，本文的主要工作是：
\begin{enumerate}
  \item 实现NTM，并将该模型应用于复杂的算法学习任务，包括加法，乘法等任务
  \item 研究课程学习对模型行性能的影响。实验结果表明，使用课程学习使NTM学到加法，乘法的算法。并且有更快的收敛速度。
  \item 研究外部记忆模块的大小对模型的影响。实验结果表明，选择合适的外部记忆大小，能提升模型的性能。
\end{enumerate}

\section{相关工作}
算法学习这个任务最早可以追溯到1967年\cite{gold1967language, angluin1983inductive}。在其漫长的研究过程中，根据模型不同或者方法不同，该任务有许多不同的名称，例如:合成程序（program synthesis）、程序归纳（program induction）、自动编程（automatic programming）等。但是其研究目标始终是让计算机有理解程序甚至自动编写程序的能力。\cite{gulwani2010dimensions} 和\cite{kitzelmann2009inductive}两篇文章从多个角度描述了算法学习的研究现状。

在该任务的诸多模型方法中，本文重点关注机器学习模型在算法学习任务中的应用，特别是\cite{graves2014neural}提出的NTM。这是第一次明确提出外部记忆增强的神经网络模型。根据这篇文章的实验结果，NTM能学习到复制，重复复制等简单算法。

之后陆续提出的模型，都是基于NTM进行改进或者参考了NTM的结构。例如：为了解决NTM不能并行和训练难度大等问题，\cite{kaiser2015neural}借鉴了Grid LSTM\cite{kalchbrenner2015grid}的思想，提出了一种高度并行的神经网络结构-神经网络GPU（Neural GPUs，N-GPUs）；\cite{zaremba2015reinforcement}结合增强学习算法和NTM，得到了增强学习神经网络图灵机（Reinforcement Learning neural turing machine, RL-NTM）；\cite{graves2016hybrid}提出的可微神经计算机（Differentiable Neural Computer, DNC），该模型对NTM做了三点改进。其余类似的结构还有记忆神经网络（Memory neural network, MNN）\cite{weston2014memory}、指针网络（Pointer Network, Ptr-Nets）\cite{vinyals2015pointer}、栈增强的循环神经网络（Stack-Argmented Recurrent Neural Network）\cite{joulin2015inferring}、层次记忆网络（Hierarchial Attentive Memory, HAM）\cite{andrychowicz2016learning}。

这些模型从并行性、外部记忆结构、寻址模式、寻址效率等方面对NTM做出改进或者提出了类似于NTM的外部记忆增强的循环神经网络结构。虽然这些模型在某些特定方向的性能得到了提升，但是模型的复杂度也随之提升，使得模型的训练难度加大。因此，这些改进是否有意义仍然有待商榷。对于算法学习这个任务来说，需要模型具备解决变长问题和解决长距离依赖的能力。因此，本文使用NTM作为模型。

\section{任务}
本文中考虑的三种算法学习任务类似于Zaremba\upcite{zaremba2016learning}\\提出的任务，分别是：复制，加法和乘法。具体的例子可以见图\ref{Fig:task}。需要强调的是：
\begin{enumerate}
  \item 对于冯诺依曼体系结构下的计算机，程序员通过编码操作CPU和寄存器可以很轻松的完成这些任务。但是对于算法学习来说，不是通过编码解决这类问题，而是使用大量输入输出样例训练NTM，使NTM自动学会这些算法。
  \item 训练模型时采用的是序列到序列（sequence-to-sequence）的框架\upcite{sutskever2014sequence}，即模型每个时间步读入一个字符，当读到终止符后，模型便不在接收输入，而是每个时间步输出一个字符。
\end{enumerate}

\begin{figurehere}
\centering
\includegraphics[width=.5\textwidth]{new_task.jpg}
\caption{三个任务的例子，从左到右分别是复制、加法和乘法。}\label{Fig:task}
\end{figurehere}

\textbf{复制} 这个任务的目的是复制输入字符。输入序列是任意长度的字符串，并以终止符结尾。这个任务需要模型能存取任意长度的信息。

\textbf{加法} 这个任务的输入序列如图\ref{Fig:task}所示，是一个求和的式子，目的是计算该算式。这个任务要求模型：1）有能力存取任意长的信息。2）发现加法中进位的概念。3）理解一位数加法的概念。本文考虑了以下两种表示方式：
\begin{enumerate}
\item 1 2 3 4 5 + 5 4
\item 1 2 3 4 5 + 0 0 0 5 4
\end{enumerate}
在实际的实验中发现，第一种表示方式处理比较困难，尤其是增加了课程学习的设计难度，而且第二种情况是包含第一种情况的。因此本文的实验采用的是第二种表示方式，即相加的两个数都有相同的长度。如果两个数的位数不同，则位数少的补零至两个数位数相同。乘法任务也是采用这种方式。

\textbf{乘法} 这个任务的目的是计算乘法算式。这个任务的复杂度要高于加法。

在模型处理序列之前，需要先对输入序列编码。以复制任务为例，假设输入序列是abc123.，通过ascii码表转换，将每个字符表示成8bit的二进制数，该序列将表示成：
\begin{enumerate}
  \item 0 1 1 0 0 0 0 1
  \item 0 1 1 0 1 1 1 0
  \item 0 1 1 0 1 1 1 1
  \item 0 0 1 1 0 0 0 1
  \item 0 0 1 1 0 0 1 0
  \item 0 0 1 1 0 0 1 1
  \item 0 0 1 0 1 1 1 0
\end{enumerate}
其中“.”表示终结符，即第七项的字符二进制表示00101110。编码完成后，第一个时间步向模型输入字符“a”的二进制表示01100001，第二个时间步将输入字符“b” 的二进制表示01101110，以此类推，每个时间步输入一个编码后的字符。除了二进制编码方式外，也可以使用其他编码方式，例如one-hot编码。在本文的实验中，复制算法采用二进制编码，加法和乘法采用one-hot编码。

\section{模型}

这部分将简要的介绍NTM。该模型的结构包括两个最基本的部分：控制器（Controller）和外部记忆模块（Memory）。图\ref{Fig:ntm_architecture}展示了一个高度抽象的NTM结构。与循环神经网络结构类似，控制器在每个时间步（time step）获得外部的一个输入，并计算得到一个输出。不同的是控制器在该时间步还会通过读头（Read Heads）从记忆模块读取一个向量作为输入，并且通过写头（Write Heads）更新记忆模块的内容。最关键的部分在于NTM的每个部分都是可微的，所以NTM可以使用梯度下降等优化算法进行训练。

\begin{figure*}
\centering
\includegraphics[width=.5\textwidth]{ntm_architecture.png}
\caption{每一个时间步$t$，控制器从外部获得输入$input[t]$，并计算得到一个输出$output[t]$。与此同时，控制器通过若干个的读头从记忆模块读取一个向量，并通过写头更新记忆模块。}
\label{Fig:ntm_architecture}
\end{figure*}

从图\ref{Fig:ntm_architecture}可以知道，NTM有四个部分，分别是控制器，记忆模块，读头和写头。其中，记忆模块是一个给定大小的矩阵；控制器可以选择前向神经网络或者循环神经网络，本文的实验使用的是LSTM\upcite{hochreiter1997long}；读头和写头是由控制器的输出确定的。除了这四个部分，模型还需要使用特定的读机制从记忆模块中读取向量，特定的写机制更新记忆模块以及特定的寻址机制确定所有读取或者更新的记忆。因此，本节的剩余部分将分别介绍读机制，写机制和寻址机制。

\subsection{读机制}

$M_{t}$表示在时间$t$，大小为$N\times M$的记忆矩阵，其中$N$表示记忆单元的个数，$M$表示每个记忆单元的大小。$w_{t}$表示在时间$t$通过读头输出的权重向量，其中$w_{t}$中第$i$维的元素$\omega_{t}(i)$ 表示第$i$个记忆单元所占的权重，并且$\omega_{t}(i)$ 满足如下约束：

\begin{equation}\label{equ:1}
  \sum_{i} \omega_{t}(i) = 1,~~~0 \leq \omega_{t}(i) \leq 1,\forall i
\end{equation}

则按照如下公式计算得到读向量
\begin{equation}\label{eqe:2}
  r_{t} \leftarrow w_{t} M_{t}
\end{equation}

\subsection{写机制}

在时间$t$，写头输出权重向量$w_{t}$，$M$维的消除向量（erase vector）$e_{t}$，$M$维的加向量（add vector）$a_{t}$，其中$e_{t}$ 的每个元素都属于区间$(0,1)$。则第$i$个记忆单元的的大小根据如下公式计算得到：
\begin{equation}\label{equ:3}
  \tilde{M}_{t}(i) \leftarrow M_{t-1}(i)[1-\omega_{t}(i)e_{t}]
\end{equation}
\begin{equation}\label{equ:4}
  M_{t}(i) \leftarrow \tilde{M}_{t}(i) + \omega_{t}(i)a_{t}
\end{equation}
\\如果存在多个写头，则将每个写头的消除向量和加向量求和得到最终消除向量和加向量，再按照公式\ref{equ:3}和公式\ref{equ:4} 进行计算。

\subsection{寻址机制}

NTM的寻址机制和现代计算机结构的寻址机制类似，但是NTM的读写机制是分布式的，即记忆的读写是按照权重分布到每一个记忆单元的，因此NTM的取址机制不是定位到某一具体的记忆单元，而是计算每个记忆单元在读写操作中所占的权重，也就是之前介绍的读机制和写机制中的权重向量$w_{t}$。

权重向量的计算结合了内容寻址和位置寻址两种寻址机制，可以将其总结为以下四个公式：
\begin{equation}\label{equ:content}
    \omega_{t}^{c} \leftarrow \frac{exp(\beta_{t}K[k_{t},M_{t}(i)])}{\sum_{j}exp(\beta_{t}K[k_{t},M_{t}(j)])}
\end{equation}
\begin{equation}\label{equ:interp}
  w_{t}^{g} \leftarrow g_{t}w_{t}^c + (1 - g_{t})w_{t-1}
\end{equation}
\begin{equation}\label{equ:shift}
  \tilde{\omega}_{t}(i) \leftarrow \sum_{j}^{N-1}\omega_{t}^{g}(j)s_{t}(i-j)
\end{equation}
\begin{equation}\label{equ:sharp}
  \omega_{t}(i) \leftarrow \frac{\tilde{\omega}_{t}(i)^{\gamma_{t}}}{\sum_{j}\tilde{\omega}_{t}(j)^{\gamma_{t}}}
\end{equation}
\\其中，公式\ref{equ:content}的$K[\cdot,\cdot]$ 表示余弦相似度函数：
\begin{equation}\label{equ:cosine}
  K[u,v] = \frac{u \cdot v}{\|u\|\|v\|}
\end{equation}
另外，公式\ref{equ:content}到公式\ref{equ:sharp}中还涉及了$k_{t}$，$\beta_{t}$，$g_{t}$，$s_{t}$，$\gamma_{t}$这五个参数。这五个参数都依赖于控制器的输出，而Graves\upcite{graves2014neural}只给定了约束条件，没有明确的定义。在本文的模型实现中，使用公式\ref{equ:k}到公式\ref{equ:b}定义这五个参数。

对于给定的控制器输出$h_{t}$，$k_{t}$，$\beta_{t}$，$g_{t}$，$s_{t}$，$\gamma_{t}$满足如下公式：
\begin{equation}\label{equ:k}
  k_{t} \leftarrow relu(h_{t})
\end{equation}
\begin{equation}\label{equ:b}
  \beta_{t} \leftarrow relu(h_{t})
\end{equation}
\begin{equation}\label{equ:g}
  g_{t} \leftarrow sigmoid(h_{t})
\end{equation}
\begin{equation}\label{equ:s}
  s_{t} \leftarrow softmax(h_{t})
\end{equation}
\begin{equation}\label{equ:b}
  \gamma_{t} \leftarrow 1 + relu(h_{t})
\end{equation}

\section{实验}

\subsection{课程学习}

课程学习是在训练模型的过程中一项重要的方法，本文实验中使用的数据集将根据输入序列的长度划分为若干组，输入序列的长度越大，则任务的难度越大。在训练时，根据数据集的难度从易到难依次用于训练。在下面的描述中，$MIN$表示训练时最小的序列长度，$MAX$表示训练时最大的序列长度。

\textbf{不用课程学习（No）} 该策略作为一个比较基准，不使用课程学习策略。即实验中每一个batch的数据随机的从$MIN$和$MAX$生成。

\textbf{普通的课程学习策略（Naive）} 开始时，生成长度为$MIN$的序列作为训练数据集，当准确率到达给定的阈值（本文的实验中该值为0.9）后，生成序列长度为$MIN + 1$的训练数据集。重复该过程，直到序列长度为$MAX$。这个策略是Bengio\upcite{bengio2009curriculum} 等人提出的。但是在本文的实验中该策略的性能有时还比不上不用课程学习。因此，在本文的实验结果中，不对该策略作比较。

\textbf{混合的课程学习（Mix）} 开始时，随机生成长度属于$[1, MIN]$的序列作为训练数据集，当准确率到达给定的阈值后，区间的右端点值加一，即随机生成长度属于$[1, MIN + 1]$的序列作为训练数据集。重复该过程，直到右端点的值等于$MAX$。该策略是Zaremba\upcite{zaremba2014learning}提出的，在本文的实验中，表现出的性能要好于不用课程学习。

\textbf{基于难度系数的课程学习（Curriculum-Rate）} 该策略是在混合的课程学习策略的基础作出的改进，当输入序列的区间是$[1, a]$时，该策略不是随机从整个区间生成数据，而是按照一定的难度系数从$[a, a]$生成数据（本文的实验中难度系数为0.5），剩余的数据从$[1, a - 1]$ 随机生成。这个策略类似于人类的学习行为，即学习一项新知识的时候，应该以新知识为主，同时还需要复制旧知识。在实验中，该策略的表现要优于不用课程学习和混合的课程学习这两个策略。

\subsection{网络参数}
在正式实验之前，本文首先用一小部分数据集对比了随机梯度下降算法以及其他两种改进的随机梯度下降算法RMSprop和Adam，并针对学习率、Mini-batch大小、权重初始化策略等超参数进行调优。最终在训练时采用RMSprop算法和表\ref{tab:superparameter}中展示的各项超参数。
\begin{table*}
\caption{网络超参数}\label{tab:superparameter}
\centering
\begin{tabular}{|c|c|}
\hline
超参数 & 参数值\\ \hline
学习率 & 0.0001 \\ \hline
momentum & 0.9 \\ \hline
Mini-batch大小 & 32 \\ \hline
LSTM隐藏层单元的个数 & 128 \\ \hline
读头数量 & 1 \\ \hline
写头数量 & 1 \\ \hline
\end{tabular}
\end{table*}

对于每个任务，还需要调整与之相关的超参数，这些参数的取值如表\ref{tab:taskparameter}所示。其中序列长度的区间表示输入序列的长度范围，需要注意的是输入序列不仅包括有效字符，还包括终结符“.”，加号“+”和乘号“$\times$”。因此，模型实际要记忆的字符个数要少于序列长度。复制算法实际要复制的序列长度最小是1，最大是10；加法中，每个加数的长度范围是1到10；乘法中，每个乘数的长度范围是1到10。记忆大小是根据任务的实际需求作出的选择，例如复制长度为10的字符序列，每个字符被编码成8bit的二进制数，理论上需要10个记忆单元，每个记忆单元的大小是8，所以记忆模块是一个$10 \times 8$的矩阵。为了研究记忆模块是否起到了理论上的作用，本文研究了不同大小的记忆模块对算法学习任务的影响。

\begin{table*}[!htbp]
\caption{任务相关参数}\label{tab:taskparameter}
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\diagbox{任务}{参数值}{超参数} & 序列长度 & 记忆大小 & 训练次数 \\\hline
复制 & [2,11] & $10\times8$& 50000 \\ \hline
加法 & [4,22] & $21\times10$& 150000\\ \hline
乘法 & [4,22] & $30\times10$& 300000\\ \hline
\end{tabular}
\end{table*}

\subsection{实验结果}
本文的程序基于深度学习框架tensorflow，代码托管在GitHub。实验中运行程序的机器使用的是美团云服务器，配置是8核的CPU，64G内存，100G固态硬盘和1核的Tesla M60 GPU。实验中的模型是NTM，各项超参数如表\ref{tab:superparameter}所示。实验任务图\ref{Fig:task}所示。对每个任务，本文都独立训练了一个模型。复制任务训练了50000个batch，加法任务训练了150000个batch，乘法任务训练了300000个batch。每个任务都分别使用三种课程学习策略训练一次。针对复制和加法任务，使用多种不同的记忆模块大小训练。

\begin{figure*}[tb]
\subfigure[]{
\begin{minipage}[t]{0.33\textwidth}%并排放两张图片，每张占页面的0.5，下同。
\centering
\includegraphics[width=\textwidth]{copy.png}
\end{minipage}
}
\subfigure[]{
\begin{minipage}[t]{0.33\textwidth}
\centering
\includegraphics[width=\textwidth]{add.png}
\end{minipage}
}
\subfigure[]{
\begin{minipage}[t]{0.33\textwidth}%并排放两张图片，每张占页面的0.5，下同。
\centering
\includegraphics[width=\textwidth]{multip.png}
\end{minipage}
}
\caption{三种课程学习在测试集上的准确率的对比。纵轴表示的是模型在测试集上的准确率，横轴表示的是训练次数。绿色的“+”连成的曲线表示不用课程学习，蓝色的虚线表示的是使用混合的课程学习策略，红色的实线表示的是使用基于难度系数的课程学习课程。从图中可以看出基于难度系数的课程学习策略训练出来的模型性能最好，而不用课程学习策略训练的模型性能最差。}\label{fig:result}
\end{figure*}

为了比较课程学习对模型性能的影响，本文对所有任务都绘制了模型在测试集上的准确率随训练轮数变化情况的曲线图，如图\ref{fig:result} 所示。准确率的定义是模型预测准确的测试样例占全部测试样例中的比例。需要注意的是，对于一个测试样例，模型会输出一个预测序列，只有整个序列都正确，才认为这个测试样例的预测是正确的。只要一个时间步的预测是不正确的，那么这个预测就是错误的。

从图\ref{fig:result}可以看出，NTM在每个任务上都能收敛，并且达到接近100\%的准确率。其中，基于难度系数的课程学习策略使模型收敛的更快。例如，在加法任务中，使用基于难度系数的课程学习策略训练的模型在120000个batch的训练后准确率达到了99\%；使用混合的课程学习策略训练的模型在150000batch训练后准确率达到了95\%；不使用的课程学习策略训练的模型在150000个batch训练后准确率只达到90\%。在乘法任务中，基于难度系数的课程学习策略和混合的课程学习策略大约在230000个batch后准确率达到99\%，但是从两条曲线可以看出混合的课程学习策略训练的模型稳定性较差；不使用课程学习策略训练的模型在300000个batch的训练准确率只有90\%。而在相对简单的复制算法任务上，课程学习策略的优势并不明显。由此可以看出，对于相对复杂的任务使用课程学习策略能有效的提升模型的性能，特别是基于难度系数的课程学习策略。


\begin{figure*}[tb]
\subfigure[]{
\begin{minipage}[t]{0.45\textwidth}%并排放两张图片，每张占页面的0.5，下同。
\centering
\includegraphics[width=\textwidth]{memory_of_copy.png}
\end{minipage}
}
\subfigure[]{
\begin{minipage}[t]{0.45\textwidth}%并排放两张图片，每张占页面的0.5，下同。
\centering
\includegraphics[width=\textwidth]{memory_of_add.png}
\end{minipage}
}
\caption{不同大小的记忆模块在复制和加法任务上的性能差异。}\ label{fig:result2}
\end{figure*}


\section{结论}
本文研究并实现了NTM，并将NTM应用于算法学习任务。通过实验说明了NTM可以学习到加法，一位数乘法等算法。此外，实验结果表明合适的课程学习策略有助于提升模型在算法学习任务上的性能。但是仍然有许多问题值得更进一步的研究，例如：1）当测试序列的长度大于训练序列的最大长度时，NTM无法作出正确的预测。2）当学习一个新算法时，都需要重新训练一个模型，能否训练一个模型可以学习多种算法。3)NTM能否存在循环神经网络的梯度消失或者梯度爆炸等问题。
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  参考文献
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\small
\bibliographystyle{unsrt}
\bibliography{reference}

\end{multicols}
\end{document}
