 \documentclass[10pt]{cctart}
 \usepackage{amsfonts}
 \setcounter{page}{1} % The number of 1st page is 1
 \usepackage{flafter}
 \usepackage{mathbbold}
 \usepackage{mathrsfs}
 \usepackage{amsmath}
 \usepackage{amssymb}
 \usepackage{latexsym}
 \usepackage{psfrag}
 \usepackage{graphicx,subfigure}
 \usepackage{fancyhdr,graphicx}
 \usepackage{multicol}
 \usepackage{enumerate}
 \usepackage{float}

 \textheight 8.4in \textwidth 7.0in \tolerance=10000
 \parskip=3pt
 \oddsidemargin -1.2truecm \baselineskip 1true mm \overfullrule=0pt
 \topmargin -0.6in
 \renewcommand{\sectionformat}{}
 \renewcommand\refname{参考文献:}
 \oddsidemargin -1.2truecm \evensidemargin 0in \makeatletter
 \setcounter{tocdepth}{3}
 \pagestyle{plain}
 \newsavebox{\mygraphic}
 \sbox{\mygraphic}{\includegraphics[totalheight=0.65in]{head.jpg}}
 \pagestyle{fancy} \topmargin=0pt \headheight=18mm \headsep=4mm
 \fancyhead{} \fancyhead[L]{\usebox{\mygraphic}}
 \fancyhead[R]{2011东南大学校庆研究生学术报告会\\}
 \fancyfoot[R]{\footnotesize{2011东南大学校庆研究生学术报告会}}
 \makeatletter
 \let\ps@tmp=\ps@fancy
 \let\ps@plain=\ps@fancy
 \makeatother \topskip=12.0pt
 \def\vp{\varphi}
 \newcounter{local}
 \newcommand{\scl}{\stepcounter{local}}
 \newtheorem{defi}{\heiti\textbf{\ \ \quad 定义}}[section]
 \newtheorem{theo}{\heiti\textbf{\ \ \quad 定理}}[section]
 \newtheorem{lem}[theo]{\heiti\textbf{\ \ \quad 引理}}
 \newtheorem{col}[theo]{\heiti\textbf{\ \ \quad 推论}}
 \newtheorem{prop}[theo]{\heiti\textbf{\ \ \quad 命题}}
 \newtheorem{remark}[theo]{\heiti\textbf{\ \ \quad 注}}
 \newcommand{\dist}{\mathrm{dist}\mbox{}}
 \newcommand\be{\begin{equation}}
 \newcommand\ee{\end{equation}}
 \newcommand\bes{\begin{eqnarray}}
 \newcommand\ees{\end{eqnarray}}
 \newcommand\bess{\begin{eqnarray*}}
 \newcommand\eess{\end{eqnarray*}}
 \newcommand\bR{{\mathbb R}}
 \newcommand{\lbl}[1]{\label{#1}}
%\mathindent 2cm
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\baselineskip 6 true mm
\date{}
\title{\heiti 基于可微神经计算的算法学习的研究
\author{{\small 王李荣~~~~ 高志强}
\\ {(\footnotesize  东南大学计算机科学与工程学院, 南京, 210096)}}}

\maketitle \hskip 1cm

\ {\heiti 摘\  \ 要} \ \
2010年, Richard A. Brualdi, Zejun Huang 以及Xingzhi Zhan提出了刻画极大非奇异partial矩阵的问题. 本文利用加
边矩阵的思想证明了, 任一阶极大非奇异partial矩阵的容许的自由元个数最小为0, 且$n\geqslant1$时, 存在自由元个数
为1的$n$阶极大非奇异partial矩阵; 并猜想, 对于$n$阶极大非奇异partial矩阵, 其可能的自由元个数为
$0, 1, 2, \cdots,\frac{n(n-1)}{2}$. 本文还给出了partial矩阵的$N_{0}-$填充的几个反例.
我们研究

\vskip 0.2cm ~{\heiti 关键词}~ ~
partial矩阵; 填充; 加边矩阵; 非奇异; partial $N_{0}-$矩阵
\vskip 1.2cm
\begin{center}
{\Large\bf   The indeterminates number of a maximal-nonsingular partial matrix and some counter examples of partial $N_{0}$ matrix}
\end{center}
\vskip 0.3cm \centerline{Jie Ren ~~~~Jianlong Chen} \vskip 0.2cm
\centerline{(Department of  Mathematics,  Southeast University,
Nanjing 210096)} \vskip 0.2cm
\noindent$\mathbf{Abstract:}$\quad
In 2010,  Richard A. Brualdi ，Zejun Huang and Xingzhi Zhan released a question about how to characterize
a maximal-nonsingular partial matrix. It has been proved, useing idea of  borderd matrix, there exists
maiximal-nonsingular partial matrix of any order, with the number of the indeterminates being zero. When $n\geqslant1$,
there also exists maximal-nonsingular partialmatrix that has only one indeterminate. Then we guess that:
for a $n$ order maximal-nonsingular partial matrix, the probable numbers of its indeterminates are $0, 1, 2, \cdots, \frac{n(n-1)}{2}$.Morever, some counter examples about the completion of partial $N_{0}-$matrix are given.\\
\vskip 0.1cm \noindent$\mathbf{Key\ words:}$\quad Partial matrix；Completion；Bordered matrix；Nonsingular; $N_{0}-$partial matrix
 \vskip 0.2cm
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{multicols}{2}

\section{引\quad 言}
执行一个程序，系统需要理解数值计算，$if$表达式，赋值，以及一系列操作的组合等许多基本操作。本文中将这些基本操作称为算法，例如加法，乘法，复制序列都被称为算法\cite{zaremba2014learning}。而通过输入和输出样例发现算法的任务，称为算法学习（Learning algorithms）。

实际上，算法学习不是一个新任务，它也被称为程序推理（Program induction），最早可以追溯到1967年\cite{gold1967language, angluin1983inductive}，是一个非常古老但却十分有意义的研究领域。这项任务有着丰富的应用场景，例如：帮助没有计算机背景的人编写程序，帮助程序员发现问题，发现新算法，帮助教学。因而在过去几十年，有许多研究者投入到该任务的研究中\cite{nordin1997evolutionary,liang2013learning,wineberg1994representation,solomonoff1964formal}，并做出了卓越的贡献。

在近些年，有研究者利用机器学习模型，尤其是循环神经网络（Recurrent Neural Network, RNN），研究该问题\cite{graves2014neural,zaremba2015reinforcement,kaiser2015neural,kurach2015neural,andrychowicz2016learning}。这些工作通过都是实验展示了利用机器学习模型可以学习到一些诸如复制，排序等简单的算法。

本文研究中使用的模型是可微神经计算机（Differentiable Neural Computer, DNC）\cite{graves2016hybrid}。DNC是一个变种的循环神经网络模型，类似于长短时记忆网路（Long Short Term Memory, LSTM）\cite{hochreiter1997long}。LSTM在很多存在长距离依赖的任务中表现出优秀的性能，例如语音识别\cite{amodei2016deep}，自然语言处理\cite{bahdanau2014neural} 等。但是在LSTM中，增加记忆单元的个数会导致模型参数个数的增加，使得模型难以训练。而DNC为了解决该问题，将记忆单元从网络中分离出来，将网络分割为可训练的控制器模块和外部记忆模块两个部分，外部记忆模块的大小将不再依赖于网络参数的个数。

本文尝试理解DNC的记忆能力，并将DNC应用于算法学习这个任务。通过实验我们发现：第一，使用课程学习，使DNC从不可收敛变成可收敛。第二，DNC在复制、加法、乘法等算法学习任务可以收敛。第三，增大外部记忆模块的大小，会加快模型的收敛速度，减少收敛时的震荡。第四，模型没有表现出泛化能力，即使用远长于训练时序列长度的样例进行测试时，模型无法正确预测输出。

\section{相关工作}
算法学习这个任务最早可以追溯到1967年\cite{gold1967language, angluin1983inductive}。在漫长的研究过程中，随着模型、方法不同，该任务有许多不同的名称，例如:合成程序（program synthesis）、程序归纳（program induction）、自动编程（automatic programming）等。 但是其研究目标始终是为了让计算机有理解程序甚至自动编写程序的能力。\cite{gulwani2010dimensions} 和\cite{kitzelmann2009inductive} 两篇论文从多个角度描述了算法学习的研究现状。

近年来，有一些研究者将机器学习方法应用于算法学习任务，并在该任务上取得了一些成果。特别是\cite{graves2016hybrid}提出的NTM模型。这是第一次明确提出外部记忆增强的神经网络模型。根据这篇文章的实验结果，NTM能学习到复制，重复排序等简单算法。

为了解决NTM不能并行和训练难度大等问题，\cite{kaiser2015neural}借鉴了Grid LSTM\cite{kalchbrenner2015grid}的思想，提出了一种高度并行的神经网络结构-神经网络GPU（Neural GPUs，N-GPUs）。该文章将N-GPUs应用于算法学习任务，实验结果表明该模型能学到二进制加法和二进制乘法。

\cite{zaremba2015reinforcement}结合增强学习算法和NTM，得到了增强学习神经网络图灵机（Reinforcement Learning neural turing machine, RL-NTM）。具体来说，该模型用离散外部记忆替代NTM的可微外部记忆，在使用增强学习训练该模型后，该模型能学到复制，重复复制等简单算法。

另一个对NTM做出改进的模型是\cite{graves2016hybrid}提出的DNC。文中讨论了NTM 的三个缺陷，并一一提出了改进方案。最后把DNC应用于一些有趣的实验中，例如在伦敦地铁中，寻找两站之间的最佳路线；学习拼图游戏等，DNC均达到98.8\%以上的准确率。

记忆神经网络（Memory neural network, MNN）\cite{weston2014memory}是稍晚于NTM提出的外部记忆增强的神经网络模型。文中将MNN应用于问答（question answering, QA）任务，实验效果要好于LSTM。与NTM不同的是，MNN的记忆单元是不可写的。

另一个记忆单元不可写的模型是指针网络（Pointer Network, Ptr-Nets）\cite{vinyals2015pointer}，文中用Ptr-Nets解决了凸包问题和旅行商问题。

除了这些模型外，还有一些研究者从外部记忆的结构，取址的效率等方面入手提出了新的模型。例如：栈增强的循环神经网络（Stack-Argmented Recurrent Neural Network）\cite{joulin2015inferring}，使用了可微的栈结构作为外部记忆。另一个类似的结构来自\cite{grefenstette2015learning}，文中研究了三种不同结构的外部记忆，包括栈，双端队列和先进先出队列。值得注意的是，这些结构的取址效率都是常数时间。

\cite{andrychowicz2016learning}提出层次记忆网络（Hierarchial Attentive Memory, HAM）。HAM用二叉树作为外部记忆，叶子节点作为记忆单元。相比于使用标准的注意力机制访存的NTM，如果外部记忆的大小是$n$的话，NTM寻址的复杂度是
$\Theta \left( n \right)$，而HAM的寻址的复杂度是$\Theta \left(log \space n \right)$。



\section{模型}

这个部分将简要的描述DNC\cite{graves2016hybrid}。DNC可以分为两个部分，一是控制器，另一个是外部记忆模块。控制器可以是前向神经网络或者循环神经网络。本文中的实验用的是LSTM，在3.1中，将简要的介绍该模型。外部记忆模块是一个给定大小的矩阵，通过两个超参数控制矩阵的大小。

\section{任务}
本文考虑了四种不同的任务，分别是：复制，加法，三个数的加法和乘法。具体的例子可以见图\ref{Fig:task}。需要强调的是，1）对于冯诺依曼体系结构下的计算机，程序员通过编码操作来CPU和寄存器可以很轻松的完成这些任务。但是对于该任务来说，不需要针对问题编码，而是使用大量数据训练DNC，使DNC学会这些算法。2）训练模型时采用的是序列到序列（sequence-to-sequence）的框架\cite{sutskever2014sequence}，即模型每个时间步读入一个字符，当读到终止符后，模型便不在接收输入，而是转为每个时间步输出一个字符。在全部实验中，都以“.”作为终止符。

\begin{figure*}[htbp]
\centering
\includegraphics[width=4.0in]{task.jpg}
\caption{四个任务的例子，从左到右分别是复制，加法，三个数的加法和乘法。}\label{Fig:task}
\end{figure*}

\textbf{复制} 这个任务的目的是复制输入字符。输入序列是任意长度的字符串，并以终止符结尾。这个任务可以测试DNC是否有能力存取任意长度的信息。

\textbf{加法} 这个任务的目的是对输入序列求和。这个任务要求模型：1）有能力存取任意长的信息。2）发现加法中进位的概念。3）理解一位数加法的概念。本文考虑了以下两种表示方式：
\begin{enumerate}[(1)]
\item 1 2 3 4 5 + 5 4
\item 1 2 3 4 5 + 0 0 0 5 4
\end{enumerate}
在实际的实验中发现，第一种表示方式处理比较困难，尤其是增加了课程学习的设计难度，而且第二种情况是包含第一种情况的。因此本文的实验采用的是第二种表示方式，即相加的两个数都有相同的长度。如果两个数的位数不同，则位数少的补零至两个数位数相同。之后的两个任务也是采用这种方式。

\textbf{三个数加法} 与上一个任务的不同点在于有三个数参与运算。相比于两个数的加法，难度在于进位的状态有三个（0,1,2）。

\textbf{一位数乘法} 这个任务是用一个一位数乘以一个多位数。这个任务的复杂度与加法类似，不过进位的状态可以是0到8里的任意一个。

\section{实验}

\subsection{网络参数}
DNC是可微的神经网络模型，在训练模型的过程中，本文首先用一小部分数据集对比了随机梯度下降算法以及其他两种改进的随机梯度下降算法RMSprop和Adam，并针对学习率、Mini-batch大小、权重初始化策略等超参数进行调优。最终在训练时采用RMSP算法和表\ref{tab:superparameter}中展示的各项超参数。在各项超参数中需要说明的是DNC的控制器使用的是只有一层隐藏层的LSTM，隐藏层的记忆单元的个数是128。

\begin{table*}
\caption{网络超参数}\label{tab:superparameter}
\centering
\begin{tabular}{|l|l|l|l| p{5cm}|}
\hline
\textbf{超参数} & \textbf{优化策略}\\ \hline

学习率 & 0.0001 \\ \hline
momentum & 0 \\ \hline
Mini-batch大小 & 32 \\ \hline
训练次数 & 100000 \\ \hline
LSTM隐藏层单元的个数 & 128 \\ \hline
权重初始化策略 & $Uniform(-r, r)$ \\ \hline
\end{tabular}
\end{table*} 

\subsection{课程学习}
课程学习是在训练模型的过程中一项重要的技术，本文使用的课程学习策略类似于\cite{zaremba2014learning}中的策略。具体来说，本文实验中使用的数据集将根据输入序列的长度划分为若干组。
\\\textbf{不用课程学习} 这作为一个比较基准，不使用课程学习策略。即我们


文献\cite{Wu,Xuan}中提到: 实验
HAM提到只要有一个bit错了，就认为整个输出序列错了
比较部分也可以参考HAM的内容
\section{结论}
文献\cite{Wu,Xuan}中提到: 结论
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  参考文献
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\small
\bibliographystyle{unsrt}
\bibliography{reference}

\end{multicols}
\end{document}
